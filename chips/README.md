# [SpiNNcloud Systems](https://spinncloud.com)

SpiNNakerは、"spiking neural network architecture"の略で、英国、マンチェスター大学のコンピュータ・サイエンス部門のAPT（Advanced Processor Technologies Research Group）の研究者たちによって開発されました。

このプロジェクトを率いるSteve Furber氏は、ARMチップの設計者で、[Human Brain Project(HBP)](https://en.wikipedia.org/wiki/Human_Brain_Project)にも深く関わっていました。HBPは、EUで2013年から10年間に渡って研究者たちの間で行われたプロジェクトで、人間の脳の構造を解明し、コンピュータ・サイエンスに生かそうするプロジェクトです。

HBPの研究者たちが注目したのは、[Spiking neural network(SNN)](https://en.wikipedia.org/wiki/Spiking_neural_network)と呼ばれる自然界のニューラルネットを模倣したArtificial Neural Networks (ANN)です。

現在、幅広く採用されているニューラルネットは、レイヤー上に配置したニューラルネットを行列演算で処理します。行列演算は並列計算が得意なGPUと相性が良く、GPUコアを大量に並べることにより大規模なニューラルネットを高速に処理することが可能です。

各レイヤーの中のニューロンから次のレイヤーのニューロンにどのくらいの強度で信号を送るかは、そのレイヤーが持つ重み付けパラメータが決めますが、この計算方法だと、たとえパラメータがゼロやゼロに限りなく近くてもGPU内の演算機を使って計算をしなければならず、スパースな行列演算の場合には、多くの電力が無駄に消費されることになります(下の図)。

![](https://satoshi.blogs.com/mag2/June2025/ann.png)

人間の脳を含めた自然界のニューラルネットは、そもそもレイヤー構造ではない上に、ニューロン間の接続は必要な場所だけにされています。

SNNは、この自然界のニューラルネットを真似ることにより、不必要な演算をしないことにより、従来型のGPUを使ったニューラルネットと比べて、より効率の良い（つまり、消費電力あたりの計算能力が高い）ニューラルネットを構築することを目指しています。

SpiNNcloud Systemsによると、今回採用されたSiNNaker2は、消費電力あたりの計算能力がGPUと比べて18倍高く、その次の世代のSpiNNextは78倍だそうです。

SpiNNaker2は、1チップあたり152個のARMコアと、GPUのような演算機能、および乱数発生機を持っており、これを48個配置したSpiNNcloudサーバーを１ラックあたり90個設置できるそうです。

１６ラックの実装だと、チップの数が69,000個、ARMコアは約1,000万個に至り、計算能力は32-bitで3PFLOPS、8-bitだと0.3EXAOPSに至ります。

# [Groq](https://groq.com)

Groqは、AIアクセラレータ市場で急速に注目を集めているスタートアップ企業です。2016年に設立され、生成AIの推論(インファレンス)に特化したチップを開発しています。

## 企業概要と最新動向
- 2024年8月に6億4,000万ドルの資金調達を完了し、企業価値は28億ドルに達成
- BlackRock主導で、Cisco Systems、Samsung Electronicsなども参加
- 累計資金調達額は10億ドル超
- 2024年末にはサウジアラビアから15億ドルの追加資金獲得と報告
- 2025年には60億ドルの企業価値での新規資金調達を検討中

## 技術仕様とパフォーマンス
### Language Processing Unit (LPU)の技術特徴
- Tensor Streaming Processor (TSP)アーキテクチャを採用
- 最大750 TOPs (INT8)、188 TFLOPs (FP16 @900 MHz)の処理能力
- 320×320行列乗算ユニット、5,120個のVector ALU搭載
- チップあたり230MB SRAM、最大80TB/sのオンダイメモリ帯域幅
- 14nmプロセス、25×29mmサイズ、動作周波数900MHz
- GroqCard™アクセラレータ価格：19,948ドル

### ベンチマーク性能 (2024年実測値)
- Llama 3 70B: 284トークン/秒 (競合他社の3-11倍高速)
- Llama 2 70B: 300トークン/秒
- Llama 2 7B: 750トークン/秒
- Mixtral 8x7B: 480トークン/秒
- 初回トークン生成時間: 0.2秒
- NVIDIAのGPUが10-30トークン/秒に対し、Groqは最大300トークン/秒を実現

## 事業展開とパートナーシップ
- GroqCloudプラットフォームに35万6,000人以上の開発者が登録 (2024年7月時点)
- 2025年Q1末までに10万8,000個以上のLPUの展開を計画
- Meta首席AI科学者のYann LeCun氏が技術アドバイザーに就任
- 元Intel foundry事業責任者Stuart Pann氏がCOOに就任
- Bell Canadaとの提携によるカナダの国家AI基盤整備

## 市場での位置づけ
Groqは独自のLPUアーキテクチャにより、従来のGPUベースのAI推論を大幅に上回る性能を実現し、NVIDIA、AMD、Intelなどの既存大手企業に対する有力な競合として急成長を続けています。特に大規模言語モデルの推論処理において、速度と効率性で業界をリードする地位を確立しています。

# [Cerebras](https://cerebras.ai)

Cerebrasは、ウェハースケールAI半導体の開発に特化したアメリカのベンチャー企業です。2015年創業で、革新的な大型AIチップ技術により急成長を遂げています。

## 企業概要と最新動向
- 2024年にIPO申請を行い、2025年Q2-Q3のナスダック上場を計画（ティッカー：CBRS）
- 2024年6月にシリーズF-1で4億ドルを調達、企業価値47億ドル
- IPOで7.5億-10億ドル調達予定、評価額70-80億ドルを想定
- 累計資金調達額：11.4億ドル超
- 2024年3月にCFIUS（対米外国投資委員会）の承認を取得

## WSE-3チップ仕様（2024年発表）
### 技術仕様
- トランジスタ数：4兆個（業界最大規模）
- AIコア数：90万個（AI最適化済み）
- チップサイズ：46,225mm²（NVIDIA H100の57倍）
- 製造プロセス：TSMC 5nmノード
- オンチップSRAM：44GB
- メモリ帯域幅：21ペタバイト/秒
- 外部メモリオプション：1.5TB、12TB、1.2PB

### パフォーマンス
- ピーク性能：125ペタフロップス
- 最大24兆パラメータのモデル学習に対応
- 2,048個のWSE-3クラスターでLlama 2 70Bを1日以内で学習可能
- Cerebras AI推論サービス：NVIDIA H100比10-20倍高速
- 前世代比2倍の性能向上（同一価格・同一消費電力）

## 財務実績と成長
### 収益実績
- 2023年売上高：2.5億ドル（前年比201%増）
- 2022年売上高：8,300万ドル
- グロスマージン：2022年11.7% → 2023年33.5%に改善
- 2024年上半期純損失：6,660万ドル（前年同期7,780万ドル）

### 主要顧客
- Group 42（UAE）：2024年上半期売上の87%を占める
- Microsoft、Meta、IBM
- 政府研究機関、グラクソ・スミスクライン、メイヨークリニック
- 2025年上半期に新たなハイパースケーラー顧客獲得予定

## 製品ラインと市場展開
### 主力システム
- CS-3：WSE-3チップ搭載システム
- Cerebras Cloud：クラウドベースのAI推論サービス
- 現在供給不足状態（需要が供給を上回る）
- 2024年完成予定クラスター総額：10億ドル超

## 業界評価と受賞
- TIME誌「2024年最優秀発明」にWSE-3が選出
- Forbes AI 50（2024年4月）
- TIME 100最も影響力のある企業（2024年5月）

## 技術的優位性
Cerebrasは従来の小型チップを複数接続する方式とは対照的に、ウェハー全体を1つのチップとして活用する革新的アプローチにより、データ移動を最小化し、AI学習・推論において圧倒的な効率性を実現しています。特に大規模言語モデルやAI基盤モデルの分野でNVIDIAに対抗する有力な選択肢として確立されています。

# [SambaNova Systems](https://sambanova.ai)

SambaNova Systems（サンバノバシステムズ）は、AIチップの設計から大規模言語モデルの開発まで手がける総合的なAIテクノロジーカンパニーです：

## 企業概要
- 設立：2017年にシリコンバレーにて設立
- バリュエーション：2021年時点で約50億ドル（約7,400億円）

## 事業内容
- DataScale®：AIに最適化されたサーバーシステム
- RDU（Reconfigurable Data Unit）：独自開発のAIプロセッサ
- SambaFlow®：ソフトウェアスタック
- 大規模言語モデル（LLM）の開発と提供
- AIソリューションのサブスクリプションサービス

## 特徴
- チップからソフトウェアまで垂直統合型のAIソリューションを提供
- GPUに依存しない独自アーキテクチャを採用
- エンタープライズ向けに最適化されたAIインフラストラクチャを展開
- オンプレミス、クラウド、ハイブリッドなど柔軟な導入形態に対応

## 主要人物
- CEO：ロドリゴ・リアン
- 共同創業者：クンレ・オルコトン（スタンフォード大学教授）
- 共同創業者：クリストファー・リー、マーシャル・チョイ

## 資金調達
- シリーズD（2021年）：6.76億ドル（約740億円）をソフトバンク・ビジョン・ファンド2がリード
- 累計調達額：10億ドル以上
- 主要投資家：Google Ventures、Intel Capital、BlackRock等

## 技術的優位性
- データセンター向けAIワークロードで従来のGPUベースシステムと比較して最大30倍の性能効率を実現
- 独自のRDUアーキテクチャにより、AIモデルの学習から推論まで効率的に処理
- 電力効率に優れ、TCO（総所有コスト）の大幅な削減を実現

## 市場での位置づけ
- NVIDIA、Intel、AMDに次ぐAIチップメーカーとして急成長
- エンタープライズAI市場において、特に金融、ヘルスケア、政府機関向けに強み
- 2023年には独自開発のLLMも発表し、AIの完全統合ソリューションプロバイダーとしての地位を確立

SambaNovaは、独自のハードウェア技術と垂直統合型のビジネスモデルを武器に、急成長するAIインフラ市場において、主要なプレイヤーとしての地位を確立しつつあります。特に、エンタープライズ向けAIソリューションの分野で、高い評価を得ています。

# [Tenstorrent](https://tenstorrent.com)

Tenstorrentは、AI向け半導体の設計に特化したカナダのスタートアップ企業です。以下にTenstorrentの主な特徴をまとめます:

- 会社概要
  - 2016年にカナダのトロントで設立された企業
  - AIアクセラレーターとRISC-Vプロセッサーの開発に注力
  - 2024年1月時点で企業価値は約20億ドル（約3000億円）に達している
  - 2023年12月にE2シリーズで3億ドルの資金調達を実施

- 技術と製品
  - AIプロセッサとRISC-VプロセッサのIP設計やチップ開発を行っている
  - 主な製品ラインには、Grayskull、Wormhole、Blackhole、E75などがある
  - データフロー型アーキテクチャを採用し、高効率なAI処理を実現
  - 2024年にはIntel 18A製造プロセスでの製品開発を発表

- 経営陣
  - CEOはジム・ケラー氏で、業界では「伝説的」「天才」と呼ばれる半導体エンジニア
  - ケラー氏は以前AMD、Apple、Tesla、Intelなどの大手企業で活躍
  - 2024年1月にはAMDの元幹部Matt Wuebbling氏をCMOとして採用

- 事業展開
  - 日本市場では2023年3月から日本法人が本格始動
  - 自動車、エンタープライズ、エッジデバイス、HPC、スーパーコンピューターなどの分野に注力
  - Microsoftと戦略的パートナーシップを締結し、AI加速器の共同開発を推進
  - 2024年にはIntelとの製造パートナーシップも発表

Tenstorrentは、AI時代の新たな半導体アーキテクチャを提案し、NVIDIAなどの既存大手に挑戦する新興企業として注目を集めています。特に2023年後半から2024年にかけて、大手テック企業との提携や新製品の発表など、急速な成長を遂げています。

# [Graphcore](https://www.graphcore.ai) - Acquired

## 会社概要
Graphcoreは、AIチップ開発分野における革新的な企業で、次世代AIコンピューティングの実現を目指しています。

## 設立背景
- 2016年にイギリスのブリストルで設立されたAI専用チップのスタートアップ企業。

## 主力製品と技術
- 「インテリジェンス・プロセッシング・ユニット(IPU)」と呼ばれるAI処理に特化したチップを開発
- IPUは従来のGPUとは異なるアーキテクチャを採用
- 大規模AIモデルの学習や推論を高速・効率的に実行可能
- ハードウェアに加え、開発者向けのソフトウェアスタックも提供

## 主な用途と市場
- AI研究開発
- データセンター
- 大規模な計算能力を必要とする分野

## 経営状況と最近の展開
- 技術力は高く評価され、NVIDIAのライバルとして期待される
- イギリスのスタートアップとして、年金基金などからの国内投資獲得に苦心
- 資金の大半を海外から調達
- 2020年末時点の企業価値は27.7億ドル
- AIチップ市場の急速な成長と必要投資規模の増大が経営課題に
- 2023年に従業員を494人まで20%削減
- ノルウェー、日本、韓国での事業operations を停止
- 資金調達の課題に直面し、収支均衡のために追加資金が必要な状況に
- 2024年7月11日、ソフトバンクグループによる買収が発表される
- 買収額は非公開
- Nigel Toon CEOは買収後も継続して経営に携わる
- ソフトバンクグループ傘下のArmを含む、グループ企業とのパートナーシップを展開予定

## SoftBankグループの投資計画
- 2024年12月、ソフトバンクグループのMasayoshi Son CEOが米国での1000億ドルの投資計画を発表
- 4年間で10万人のAI関連雇用創出を目指す
- 投資資金はVision Fund、資本プロジェクト、Arm Holdings等から調達予定
- OpenAIへの15億ドル投資を含む、既存の投資計画も一部含まれる可能性
- 2016年の500億ドル投資・5万人雇用創出計画の後続となる投資

# [Etched](https://www.etched.com)

Etchedは、2022年に設立されたAIチップのスタートアップで、トランスフォーマーモデルに特化したASIC（Application-Specific Integrated Circuit）を開発しています。以下は会社の主な特徴です。

- 設立年：2022年
- 所在地：アメリカ・サンフランシスコ
- 製品：Sohuという名前のチップを開発。これはトランスフォーマーモデル（GPT-4、Claude、Stable Diffusionなど）を効率的に動作させるために設計されており、特に大規模言語モデルの推論や学習に最適化されています。TSMCの5nmプロセスで製造されており、量産は2024年後半から開始予定とされています。
- 競合：NVIDIAのGPU（特にH200やB200 Blackwell）に対抗する製品を開発しており、Sohuチップは従来のGPUと比較して大幅に高いパフォーマンスと低コストを実現するとされています。
- 技術的強み：Sohuチップは、汎用GPUがサポートする多様なAIモデルではなく、トランスフォーマーに特化しているため、より多くの計算資源を効率的に使用できるのが特徴です。さらに、最新のトランスフォーマーアーキテクチャに最適化された独自の行列演算ユニットを搭載しています。
- パフォーマンス：1台のSohuサーバーが160台のH100 GPUに相当する計算能力を持つと主張されています。特に、推論タスクにおいて高い効率性を実現しています。
- 資金調達：シリーズAで1億2,000万ドルの資金を調達し、主要な投資家にはPayPal元CEOのPeter Thielなどがいます。2024年には追加の資金調達を実施し、評価額は大幅に上昇したとされています。

このスタートアップは、AI技術の進展に伴い、トランスフォーマーモデルの利用が増えると予測し、その需要に応えるためのチップ開発を加速させています。特に、エッジデバイスでの推論性能の向上と、大規模言語モデルのファインチューニングの効率化に注力しているとされています。

# [MatX](https://matx.com/)

## ミッション
大規模AIモデルを物理法則が許す限り効率的に実行し、AIの品質と利用可能性を数年先に進めることを目指しています。これにより、誰もが専門医、家庭教師、コーチ、アドバイザー、アシスタントなどの大規模なAIスタッフを持てる、より幸せで豊かな世界を実現します。

## 製品の特徴
- 大規模AIモデル専用のハードウェアを提供
- 性能対コスト比を最優先し、大量の事前学習や本番推論に最適化
- 70Bクラスのモデルで10ms/token未満の競争力のある遅延を実現
- 最低7B（理想的には20B以上）のパラメータを持つTransformerベースのモデルに対応
- 数千から数百万の同時ユーザーに対応可能

## 目標と可能性
- 世界最高のAIモデルを3〜5年早く実現させる
- 個人研究者が毎日7Bクラスのモデルを、月に複数回70Bクラスのモデルを学習可能に
- スタートアップ企業でもGPT-4クラスのモデルを学習し、ChatGPTレベルのトラフィックに対応可能に

## 創業チーム

### Reiner Pope (共同創業者 & CEO)
10年以上にわたり、MLチップからLLMまで、分散システムインフラストラクチャの高性能ソフトウェアとハードウェアの開発に従事。
- Google PaLMのEfficiency Leadとして、世界最速のLLM推論ソフトウェアを設計・実装
- GoogleのTPU v5eの考案とLLM向け最適化に貢献
- GoogleのMLチップのアーキテクトとして、当時最速のMLチップの設計に携わる
- MLチップのコンパイラリードとして、MLチップの最も困難な課題の1つ（ソフトウェア環境の構築）を解決
- 11件の特許を保有

### Mike Gunter (共同創業者 & CTO)
複数回の起業経験を持ち、28年のハードウェアアーキテクチャ経験と12年のML経験を有する。
- GoogleのMLチップのチーフアーキテクトとして、当時最速のMLチップを設計
- Google TPUのアーキテクトとして、コア計算ユニットの最適化を開発
- プロセッサ、無線通信、グラフィックス、MLなど11チップの設計と実装経験
- 23件の特許を保有

### Avinash Mani (Chief Development Officer, Silicon)
25年以上のシリコンとソフトウェアの製品開発および世界クラスのエンジニアリングチーム構築経験。
- 12以上のチップ製品をコンセプトから大量顧客デプロイメントまで実現
- Amazon、Innovium、Broadcomで100人以上のエンジニアチームを構築・指揮
- Innoviumの創業チームメンバーとして、10億ドル以上の価値創出に貢献
- 10件以上の特許を保有

## 投資家
2,500万ドルの資金を調達。主な投資家：
- Daniel GrossとNat Friedman（リード投資家）
- Rajiv Khemani（Intel、Cavium、Innovium、AIspace、Auradine）
- Outset Capital
- SV Angel
- Homebrew
- Amit Singh（Oracle、Google、Palo Alto Networks）
- 著名なLLMおよびAI研究者（Irwan Bello、James Bradbury、Aakanksha Chowdhery、William Fedus、David Ha）
- Swyx（Latent.Space、AI.Engineer、AWS、Netlify、Temporal）

MatXは、大規模AIモデルの性能を飛躍的に向上させ、AIの進歩を加速させることを目指している革新的な企業です。チームは、GoogleやAmazonなどでMLチップ、MLコンパイラ、MLモデルの開発経験を持つ業界のベテランと、優秀な新卒エンジニアで構成されています。

# [FuriosaAI](https://furiosa.ai)

- 2017年創業の韓国・ソウル発AI半導体スタートアップ
- 創業者はAMD、Qualcomm、Samsung出身のエンジニアたち
- 高性能かつ省電力なAI推論チップを開発し、データセンター向けに展開

# 主力製品：RNGD（Renegade）
- 第2世代AI推論アクセラレータ
- TSMCの5nmプロセスを使用、HBM3メモリ48GB搭載
- 消費電力は150～180Wと高効率（Nvidia H100の約1/4）
- 独自アーキテクチャ「TCP（Tensor Contraction Processor）」を採用
- FP8で最大512TFLOPSの演算性能
- GPT-JやLlamaなど10Bパラメータ級のモデルで毎秒2000～3000トークン処理可能

## 特徴・優位性
- 高い電力効率：同等のGPUに比べて約2.25倍の推論性能/Wを実現
- 高密度実装：同一ラックあたり3.75倍のトークン処理能力
- Nvidiaに対抗する設計思想：GPU依存からの脱却を狙う

## 主なパートナー・導入事例
- LG AI研究所が大規模評価を経て採用（EXAONEプラットフォームで稼働）
- 電子機器、金融、通信、バイオなどの分野で実運用開始
- TSMC、SK hynix、Samsung、ASUSと連携

## 資金調達・買収拒否
- これまでに1億ドル以上の資金調達
- 2025年、Meta（Facebook）が8億ドルで買収を提案 → FuriosaAIは独立性維持のため拒否
- CEOのパイク・ジュン氏は「持続可能なAIコンピューティング」を目指すとコメント

# [Hailo](https://hailo.ai/)

Hailoは、エッジAI向けの高性能プロセッサを開発するイスラエルのAIチップメーカーです。以下にHailoの主な特徴をまとめます:

- 2017年に設立されたスタートアップ企業です。
- エッジデバイス向けの低消費電力・高性能AIプロセッサを開発しています。
- 主力製品は「Hailo-8」というAIチップで、26TOPSの処理性能を持ちます。
- 自動運転車、監視カメラ、産業用ロボットなどのエッジAIアプリケーションをターゲットにしています。
- 独自のアーキテクチャにより、同等性能の他社製品と比べて大幅な低消費電力化を実現しています。
- AIモデルの最適化ツールチェーンも提供し、エッジデバイスでの効率的なAI実行を支援しています。
- 2020年に日本法人を設立し、日本市場にも本格参入しています。

## 技術的アプローチ

Hailoは、Domain-Specific Dataflow（ドメイン特化型データフロー）アーキテクチャを採用しています。この技術的選択の背景には以下の考えがあります：

- 従来の命令セットアーキテクチャ（ISA）では、実際の演算に使用される電力は全体の10%未満であり、残りは命令キャッシュへのアクセスや制御に消費されています。
- GPUも命令セットベースのマシンであり、同様の限界に直面しています。
- ディープラーニング処理に特化したドメイン特化型アーキテクチャを採用することで、電力効率を大幅に向上させています。
- MAC（Multiple and Accumulate）演算に基づく単純な繰り返し計算を効率的に実行できる設計を実現しています。
- ソフトウェア（ディープラーニングコンパイラ）とハードウェアの共同設計により、柔軟性と効率性のバランスを取っています。

## エッジAIの進化とビジョン

Hailoは、エッジAIの進化を以下の3つの段階で捉え、その実現に取り組んでいます：

1. 知覚AI（Perceptive AI）
   - 機械による視覚的世界の認識と解釈
   - 物体検出やインスタンスセグメンテーションなどの高度な認識タスク
   - 監視、安全モニタリング、法執行などへの応用

2. 強化AI（Enhancive AI）
   - 画像品質の向上や視覚データの強化
   - 低光量環境対応、HDR、デジタルズーム、局所的トーンマッピングなど
   - より明確で詳細な視覚データの実現

3. 生成AI（Generative AI）
   - エッジでの生成AI実現に向けた取り組み
   - 以下の利点を提供：
     - コスト効率：サブスクリプション不要の所有モデル
     - 低遅延：即時的な結果生成
     - 常時接続性：継続的なアクセス保証
     - プライバシー保護：エッジでのデータ処理
     - 持続可能性：低消費電力と低炭素フットプリント

Hailoは、エッジAIの実用化に向けて注目を集めているAIチップベンチャーの1つとして、特に消費電力効率を重視する現代のコンピューティングニーズに対応した革新的なアプローチを提供しています。個人用コンピュータや車載インフォテインメントセンターなど、様々なエッジデバイスでの生成AIの実行を可能にし、より身近でクリエイティブなAI活用の未来を目指しています。

# [MYTHIC](https://mythic.ai/)

MYTHICは、エッジAI（端末側での人工知能処理）に特化した高性能アナログコンピューティング企業です。以下にMYTHICの主な特徴を説明します：
- 主要製品
    - M1076 Analog Matrix Processor: 単一チップで最大25 TOPSの処理能力を持つ、高性能エッジAIアプリケーション向けプロセッサ
    - MM1076 M.2 M Key Card: エッジデバイスやエッジサーバー向けの高性能かつ省電力なAI推論を可能にするカード
- 技術的特徴
    MYTHICは、従来のデジタルコンピューティングとは根本的に異なるアーキテクチャを採用しています：
    - アナログコンピューティング: デジタル方式とは異なり、AIパラメータをプロセッサ内に直接保存することで、メモリのボトルネックを解消
    - コンピュート・イン・メモリ: この技術により、現在は不可能または非実用的なAIアプリケーションの実現を目指しています
- パフォーマンス
    MYTHICの技術は、標準的なAI推論用デジタルチップと比較して以下のような優位性を持っています：
    - コスト効率が10倍向上
    - 消費電力が3.8倍削減
    - 処理速度が2.6倍向上
- 応用分野
    MYTHICの技術は以下の分野での活用が期待されています：
    - ロボティクス
    - 防衛
    - セキュリティ
    - スマートホーム
    - 消費者向けデバイス

MYTHICは、コスト効率と省エネルギー性に優れたエッジAIコンピューティングパワーを提供することで、AIの可能性を拡大することを目指しているベンチャー企業です。

# [blaize](https://www.blaize.com/)

Blaizeは、AIエッジコンピューティングに特化したチップベンチャー企業です。以下にBlaizeの特徴をまとめます：
- 目的：エッジでのAIアプリケーションのコンピューティングと製品化のニーズを満たすことを目指しています。
- 製品：AIコンピューティング向けのハードウェアとソフトウェアを開発・提供しています。主な製品には、PathfinderプラットフォームとXplorerプラットフォームがあります。
- ソフトウェア：Blaize AIソフトウェア・スイートを提供し、開発者がエッジAI製品を効率的に開発できるようサポートしています。
- 技術的特徴：低消費電力、低複雑性、低コストを実現し、エッジでのAI採用を促進することを目指しています。
- 応用分野：産業、スマートシティなど、様々な分野でのエッジAIアプリケーションに対応しています。
- グローバル展開：日本市場にも進出しており、エヌエスアイテクスを正規販売代理店として展開しています。

Blaizeは、データセンターからエッジへのAIコンピューティングの移行を促進し、より実用的で採算性の高いエッジAIソリューションの実現を目指している革新的なチップベンチャー企業と言えます。

# [Horizon Robotics](https://en.horizon.cc/core-technologies/)

Horizon Roboticsは、自動運転向けのAIチップ開発に特化した中国のスタートアップ企業です。主な特徴は以下の通りです:
- 2015年に設立され、自動運転車やロボット向けのAIチップ開発に注力しています。
- 主力製品は「Journey」シリーズの車載AIチップで、自動運転やADAS(先進運転支援システム)向けに開発されています。
- Journey 2、Journey 3、Journey 5など複数世代のチップを開発・発表しており、性能を継続的に向上させています。
- これらのチップは高性能かつ低消費電力を特徴としており、自動車メーカーに採用されています。
- BYD、長安汽車、広州汽車など、多くの中国自動車メーカーと提携関係を築いています。
- AIチップだけでなく、ソフトウェアプラットフォームも提供し、トータルソリューションを展開しています。
- 自動運転分野で、MobileyeやNVIDIAなどのグローバル企業と競合しています。
- 2030年までに自動運転用AIチップの世界シェア30%獲得を目指すなど、野心的な目標を掲げています。

Horizon Roboticsは、中国国内の自動車産業と密接に連携しながら、自動運転技術の発展に重要な役割を果たしている企業といえます。

# [Mobileye](https://www.mobileye.com)

Mobileyeは、先進運転支援システム(ADAS)および自動運転技術の開発・提供を行う企業です。主な特徴は以下の通りです:
- 1999年にイスラエルで創業された企業で、現在はIntelの子会社です。
- 画像認識技術を用いた単眼カメラシステムを主力製品とし、衝突防止や車線逸脱警報などの機能を提供しています。
- 世界の多くの自動車メーカーに技術を提供しており、ADASの分野でリーディングカンパニーとして知られています。
- EyeQと呼ばれる専用プロセッサーを開発し、継続的に性能を向上させています。
- 自動運転技術の開発にも注力しており、完全自動運転の実現を目指しています。
- MaaS(Mobility as a Service)分野にも進出し、自動運転タクシーなどのサービス展開も計画しています。
- 2022年10月にナスダックに上場し、時価総額は約3兆円を記録しました。

Mobileyeは、画像認識技術を核に、自動車の安全性向上と自動運転の実現に向けて幅広い事業を展開している企業といえます。

---

# [celestial](https://www.celestial.ai/)

Celestial AIは、次世代のチップ技術を開発しているベンチャー企業です。以下にCelestial AIの主な特徴をまとめます:
- 光ベースのチップ開発: 電気ではなく光をベースとしたチップを構築しています。これにより、従来の電気ベースのチップよりも高いエネルギー効率を実現することを目指しています。
- 量子コンピューティング技術: 量子コンピューティング関連の技術も開発しているようです。
- MIT関連: 米国マサチューセッツ工科大学(MIT)発のベンチャーキャピタル「The Engine Ventures」の投資先企業の一つです。
- 革新的技術: 従来のシリコンチップとは異なるアプローチで、次世代のコンピューティング技術を追求しています。
- エネルギー効率の向上: 光ベースの技術を用いることで、データセンターなどでの電力消費を大幅に削減できる可能性があります。

Celestial AIは、コンピューティング技術の未来を変える可能性を秘めた革新的なスタートアップとして注目されています。しかし、具体的な製品化や商業化の段階については、現時点で詳細な情報が限られています。

# [LightMatter](https://lightmatter.co)

Lightmatterは、光を使用したAI処理用チップを開発しているスタートアップ企業です。主な特徴は以下の通りです:
- 設立背景: 2017年にマサチューセッツ工科大学(MIT)からスピンオフして設立されました。
- 技術の特徴: 従来の電気信号ではなく、光信号を使用してAI処理を行うシリコンチップを開発しています。
- 期待される利点:
    - 処理速度: 従来の電子チップと比べて1.5〜10倍高速な処理が可能とされています。
    - 省電力: 従来チップの6分の1程度の電力消費で済むとされています。
    - 冷却効率: 発熱が少なく、冷却が容易です。
- 製品ライン:
    - Envise: 世界初の光コンピューティングプラットフォーム
    - Passage: 高帯域・高効率のチップ間通信を可能にする光インターコネクト
    - Idiom: 深層学習モデル用のソフトウェアツール
- 目標: AIコンピューティングの急成長を可能にしつつ、環境への影響を最小限に抑えることを目指しています。
- 資金調達: 2023年5月に154百万ドルのシリーズC資金調達に成功し、Google Venturesなども投資しています。

Lightmatterの技術は、増大するAI処理需要に対応しつつ、データセンターの電力消費や冷却の課題を解決する可能性があるとして注目されています。

# [Rain Neuromophics](https://rain.ai)

Rain AIは、人工知能(AI)のための最もエネルギー効率の高いハードウェアを開発している企業です。この会社は、AIの計算能力を向上させながら、エネルギー消費を抑える革新的なソリューションの創造に取り組んでいます。Rain AIの主な特徴と目標は以下の通りです：
- エネルギー効率：従来のAIハードウェアと比較して、より少ないエネルギーで高性能な計算を実現することを目指しています。
- AIに特化：特にAI処理に最適化されたハードウェアの開発に注力しています。
- 革新的技術：既存のAIハードウェアの限界を超えるための新しい技術やアプローチを探求しています。
- 持続可能性：エネルギー効率の向上は、環境負荷の低減にもつながり、より持続可能なAI技術の実現に貢献します。

Rain AIの取り組みは、急速に発展するAI技術の分野において、計算能力とエネルギー効率のバランスを取るという重要な課題に対する解決策を提供することを目指しています。この企業の成功は、将来のAI応用の可能性を広げるとともに、テクノロジー産業の環境負荷軽減にも寄与する可能性があります。

# [Ceremorphic](ceremorphic.com)

Ceremorphicは、持続可能なコンピューティングインフラストラクチャを提供する企業です。以下に、Ceremorphicの主な特徴と提供するソリューションについて簡単に解説します：
- エネルギー効率の高いAIスーパーコンピューティング：AIと生成AIアプリケーションの急速な成長に対応するため、Ceremorphicは高性能なAIトレーニング用スーパーコンピューティング市場を変革する革新的なアーキテクチャを開発しています。
- BioCompDiscoverX™：独自のアナログとAI技術を基にした、創薬設計のためのアーキテクチャを提供しています。これにより、個別化医療の実現を目指しています。
- 先進的な技術：
    - 200以上の特許
    - 3nmプロセス技術
    - ThreadArch®と呼ばれる独自のマルチスレッド処理
    - 量子耐性セキュリティ

Ceremorphicは、高性能コンピューティングとAI技術の分野で、エネルギー効率と持続可能性に焦点を当てた革新的なソリューションを提供する企業として位置付けられています。

# [Preferred Networks](https://www.preferred.jp)

Preferred Networks (PFN)は、AIソフトウェア開発企業として知られていますが、近年AIチップの独自開発にも注力しています。以下にPFNのAIチップ開発の主なポイントをまとめます：
- 独自AIチップ「MN-Core」の開発: 2016年から開発を開始し、現在第2世代まで進んでいます。高性能・高効率を目指し、ソフトウェア会社ならではの設計アプローチを採用しています。
- 開発の背景: 計算資源の重要性と将来的な調達リスクを早期に認識し、自社でコントロールする必要性を感じたことが開発のきっかけです。
- 特徴と強み: ソフトウェアに強い企業の視点を活かした設計思想を持ちます。省エネ性能に優れ、これを競争力としています。
- 今後の展開: 2026年に推論処理用の半導体の製品化を予定しています。より高性能な半導体の開発や、計算力そのものを市場に提供することも検討中です。
- 業界での位置づけ: NVIDIAなど既存の大手に対抗し、独自の立ち位置を確立しようとしています。
- 製造パートナー: 台湾のTSMCに生産を委託しています。

PFNは、AIソフトウェア開発の知見を活かしながら、ハードウェア（AIチップ）開発にも積極的に取り組むことで、AI技術の総合的な発展を目指しています。

---

# [ペジーコンピューティング](https://www.pezy.co.jp)

ペジーコンピューティング（PEZY Computing）は、2010年に設立された日本のベンチャー企業で、スーパーコンピューター向けのプロセッサ開発を主な事業としていました。
主な特徴は以下の通りです:
- メニーコア技術: 1つのチップに多数のCPUコアを搭載するメニーコアプロセッサの開発に特化していました。
- 高い技術力: 2015年に開発したスパコン「Shoubu」がGreen500ランキング（省エネ性能ランキング）で世界1位を獲得するなど、優れた技術力を示しました。
- 液浸冷却技術: 「ZettaScaler」シリーズという高効率な冷却システムを開発し、電力効率の向上を図りました。
- 幅広い応用: スーパーコンピューター向けだけでなく、AI、機械学習、ビッグデータ解析など様々な分野での応用を目指していました。

しかし、2017年に創業者の齊藤元章氏が新エネルギー・産業技術総合開発機構(NEDO)からの助成金約6.5億円を不正に受給した疑いで逮捕される事件が発生しました。齊藤氏は2020年に懲役5年の実刑判決を受けましたが、会社自体は存続し、経営陣を交代して事業を継続しています。

ペジーコンピューティングは、高い技術力を持つベンチャー企業として注目を集めましたが、同時に公的資金の不正受給という問題も起こし、ベンチャー企業の成長と倫理的な経営の難しさを示す事例となりました。

# [LeapMind](https://leapmind.io) - Closed

LeapMindは、2012年に設立された日本のAIチップベンチャー企業でした。同社は主に以下の特徴を持っていました:
- 主な事業内容
    - エッジデバイス向けの超低消費電力AIアクセラレータIP「Efficiera」の開発・販売
    - ディープラーニングモデルの軽量化・高速化技術の開発
- 技術的特徴
    - AIモデルの量子化技術に強みを持ち、1ビットや2ビットまで圧縮することでモデルサイズを100分の1以下に削減
    - エッジデバイス上でAI処理を完結させる技術を追求
- 主な実績
    - NTTデータ社とのドローンによる電線追跡自律制御プロジェクト
    - 手話通訳アプリケーションの開発
    - NECプラットフォームズやマクセルフロンティア、オカムラなどによるEfficieraの採用
- 企業の軌跡
    - 2012年の創業以来、エッジAIデバイスの汎用化に取り組む
    - 2016年から2019年にかけて、合計約50億円の資金調達を実施
    - 2023年10月に、高性能AI用半導体チップの開発開始を発表
    - 2024年7月31日付で解散を発表

LeapMindは、AIの実用的な社会実装を目指し、ハードウェアとソフトウェアの両面からアプローチを行っていましたが、12年間の挑戦の末、2024年に解散することとなりました。

--
# [Untether AI](https://www.untether.ai)

Untether AIは、2018年にトロントで設立された企業で、高性能かつエネルギー効率の良いAIインファレンス加速チップとハードウェアの開発を専門としています。同社の革新的な技術は、メモリの近くに処理を配置することでデータの移動と消費電力を最小限に抑える「メモリ近接演算」に焦点を当てています。

## 主な特徴
- **メモリ近接演算アーキテクチャ**：この独自のアプローチにより、計算のボトルネックを解消し、AI計算の効率性を向上させています。
- **エネルギー重視の設計**：Untether AIの製品は、エッジデバイスからクラウドコンピューティングまで、AIインファレンス加速において優れたエネルギー効率を提供します。
- **汎用性**：様々なニューラルネットワークモデルをサポートし、複数の産業分野に適用可能です。

## 製品とアプリケーション
Untether AIの主力製品には以下が含まれます：
- **tsunAImi アクセラレータカード**：runAIデバイスを搭載し、インファレンス加速において記録的なエネルギー効率と計算密度を実現します。
- **AIインファレンスプロセッサとチップ**：機械視覚や音声認識機能を含むインファレンス処理を可能にします。

同社の技術は以下の分野での応用が期待されています：
- 銀行・金融サービス
- 自然言語処理
- 自動運転車
- スマートシティと小売ソリューション
- エッジコンピューティングデバイス

## 資金調達と成長
Untether AIは以下を含む大規模な投資を獲得しています：
- 2021年にTracker Capital ManagementとIntel Capitalが主導した1億2500万ドルの資金調達ラウンド
- 2019年にIntel Capitalから1300万ドルのシリーズAラウンドを含む初期投資

この資金調達により、Untether AIは製品の市場展開を拡大し、次世代AI計算ソリューションの開発を加速する態勢が整いました。
